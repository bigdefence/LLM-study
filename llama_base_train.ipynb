{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"100suping/korean_unlabeled_web_text\", split=\"train\")\n",
    "\n",
    "max_samples=10000\n",
    "dataset=dataset.select(range(min(len(dataset),max_samples)))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", text)\n",
    "    text = re.sub(r\"\\n+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "output_file='korean_webtext_cleaned.txt'\n",
    "with open(output_file, 'w') as f:\n",
    "    for example in dataset:\n",
    "        text = clean_text(example['text'])\n",
    "        f.write(text + '\\n')\n",
    "print(f\"Saved cleaned text to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('LGAI-EXAONE-3.5-7.8B-Instruct')\n",
    "VOCAB_SIZE = len(tokenizer)\n",
    "print(f\"Vocab size: {VOCAB_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self,txt,max_len,stride):\n",
    "        token_ids=tokenizer(txt)\n",
    "        self.inputs_ids=[]\n",
    "        self.target_ids=[]\n",
    "        for i in range(0,len(token_ids)-max_len,stride):\n",
    "            self.inputs_ids.append(torch.tensor(token_ids[i:i+max_len]))\n",
    "            self.target_ids.append(torch.tensor(token_ids[i+1:i+max_len+1]))\n",
    "    def __len__(self):\n",
    "        return len(self.inputs_ids)\n",
    "    def __getitem__(self,idx):\n",
    "        return self.inputs_ids[idx],self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self,head_dim,base=10000,max_seq_len=2048):\n",
    "        super().__init__()\n",
    "        self.dim=head_dim//2\n",
    "        theta=1.0/(base**(torch.arange(0,self.dim,2).float()/self.dim))\n",
    "        seq_pos=torch.arange(max_seq_len,dtype=torch.float)\n",
    "        freqs=torch.einsum(\"i,j->ij\",seq_pos,theta)\n",
    "\n",
    "        self.register_buffer('sin_table',torch.sin(freqs),persistent=False)\n",
    "        self.register_buffer('cos_table',torch.cos(freqs),persistent=False)\n",
    "    def forward(self,x):\n",
    "        seq_len=x.size(2)\n",
    "        return self.apply_rope(x,seq_len,start_pos)\n",
    "    def apply_rope(self,x,seq_len,start_pos=0):\n",
    "        x_rope,x_pass=x.split(self.dim,dim=-1)\n",
    "        x1,x2=x_rope.chunk(2,dim=-1)\n",
    "        sin_table=self.sin_table[start_pos:start_pos+seq_len].unsqueeze(0).unsqueeze(0)\n",
    "        cos_table=self.cos_table[start_pos:start_pos+seq_len].unsqueeze(0).unsqueeze(0)\n",
    "        x1_rot=x1*cos_table-x2*sin_table\n",
    "        x2_rot=x1*sin_table+x2*cos_table\n",
    "        x_rope=torch.cat((x1_rot,x2_rot),dim=-1)\n",
    "        return torch.cat((x_rope,x_pass),dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GQA(nn.Module):\n",
    "    def __init__(self,d_in,d_out,n_query_heads,n_kv_heads,max_seq_len):\n",
    "        super().__init__()\n",
    "        self.n_query_heads=n_query_heads\n",
    "        self.n_kv_heads=n_kv_heads\n",
    "        self.n_rep=n_kv_heads//n_query_heads\n",
    "        self.head_dim=d_out//n_query_heads\n",
    "        self.d_out=d_out\n",
    "        self.q_proj=nn.Linear(d_in,d_out,bias=False)\n",
    "        self.k_proj=nn.Linear(d_in,d_out,bias=False)\n",
    "        self.v_proj=nn.Linear(d_in,d_out,bias=False)\n",
    "        self.o_proj=nn.Linear(d_out,d_out,bias=True)\n",
    "        self.dropout=nn.Dropout(0.1)\n",
    "\n",
    "        causal_mask=torch.triu(torch.ones(max_seq_len,max_seq_len),diagnal=1).bool()\n",
    "        self.register_buffer('causal_mask',causal_mask)\n",
    "\n",
    "        self.rope=RotaryEmbedding(self.head_dim,max_seq_len=max_seq_len)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        b,seq_len,_=x.shape\n",
    "        q=self.q_proj(x)\n",
    "        k=self.k_proj(x)\n",
    "        v=self.v_proj(x)\n",
    "\n",
    "        q=q.view(b,seq_len,self.n_query_heads,self.head_dim).transpose(1,2)\n",
    "        k=k.view(b,seq_len,self.n_kv_heads,self.head_dim).transpose(1,2)\n",
    "        v=v.view(b,seq_len,self.n_kv_heads,self.head_dim).transpose(1,2)\n",
    "\n",
    "        if past_kv is not None:\n",
    "            past_k,past_v=past_kv\n",
    "            k=torch.cat((past_k,k),dim=2)\n",
    "            v=torch.cat((past_v,v),dim=2)\n",
    "\n",
    "        kv_seq_len=k.size(2)\n",
    "        q_seq_len=q.size(2)\n",
    "\n",
    "        q=self.rope(q,start_pos=0)\n",
    "        k=self.rope(k,start_pos=0)\n",
    "\n",
    "        attn_scores=torch.zeros(b,self.n_query_heads,q_seq_len,kv_seq_len).to(x.device)\n",
    "        for qh in range(self.n_query_heads):\n",
    "            kv_head_idx=qh//self.n_rep\n",
    "            scores=torch.matmul(q[:, qh:qh+1],k[:, kv_head_idx:kv_head_idx+1].transpose(-2,-1))\n",
    "            attn_scores[:, qh:qh+1] = scores / math.sqrt(self.head_dim)\n",
    "        causal_mask_slice=self.mask[:q_seq_len,:kv_seq_len].unsqueeze(0).unsqueeze(0)\n",
    "        attn_scores=attn_scores.masked_fill(causal_mask_slice==0,float('-inf'))\n",
    "\n",
    "        attn_weights=F.softmax(attn_scores,dim=-1)\n",
    "        attn_weights=self.dropout(attn_weights)\n",
    "\n",
    "        context=torch.zeros(b,self.n_query_heads,q_seq_len,self.head_dim).to(x.device)\n",
    "        for qh in range(self.n_query_heads):\n",
    "            kv_head_idx=qh//self.n_rep\n",
    "            context[:,qh:qh+1]=torch.matmul(attn_weights[:,qh:qh+1],v[:,kv_head_idx:kv_head_idx+1])\n",
    "        context=context.transpose(1,2).contiguous().view(b,seq_len,self.d_out)\n",
    "        \n",
    "        out=self.o_proj(context)\n",
    "        if use_cache:\n",
    "            return out,(k,v)\n",
    "        else:\n",
    "            return out,(None,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self,dim,eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps=eps\n",
    "        self.weight=nn.Parameter(torch.ones(dim))\n",
    "    def forward(self,x):\n",
    "        normed=x*torch.rsqrt(x.pow(2).mean(dim=-1,keepdim=True)+self.eps)\n",
    "        return self.weight*normed\n",
    "class SwiGLU(nn.Module):\n",
    "    def forward(self,x1,x2):\n",
    "        return F.silu(x1)*x2\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,emb_dim,expansion_factor=4):\n",
    "        super().__init__()\n",
    "        self.inner_dim=emb_dim*expansion_factor\n",
    "        self.fc1=nn,Linear(emb_dim,2*self.inner_dim,bias=True)\n",
    "        self.act=SwiGLU()\n",
    "        self.fc2=nn.Linear(self.inner_dim,emb_dim,bias=True)\n",
    "        self.dropout=nn.Dropout(0.1)\n",
    "    def forward(self,x):\n",
    "        x=self.fc1(x)\n",
    "        x1,x2=x.chunk(2,dim=-1)\n",
    "        x=self.act(x1,x2)\n",
    "        x=self.fc2(x)\n",
    "        x=self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,emb_dim,n_query_heads,n_kv_heads,max_seq_len):\n",
    "        super().__init__()\n",
    "        self.norm1=RMSNorm(emb_dim)\n",
    "        self.att=GQA(emb_dim,emb_dim,n_query_heads,n_kv_heads,max_seq_len)\n",
    "        self.att_drop=nn.Dropout(0.1)\n",
    "        self.norm2=RMSNorm(emb_dim)\n",
    "        self.mlp=MLP(emb_dim)\n",
    "        self.mlp_drop=nn.Dropout(0.1)\n",
    "    def forward(self,x):\n",
    "        hidden=self.norm1(x)\n",
    "        attn_out,(k,v)=self.att(hidden,past_kv=past_kv,use_cache=use_cache)\n",
    "        attn_out=self.att_drop(attn_out)\n",
    "        x=x+attn_out\n",
    "        hidden=self.norm2(x)\n",
    "        mlp_out=self.mlp(hidden)\n",
    "        mlp_out=self.mlp_drop(mlp_out)\n",
    "        x=x+mlp_out\n",
    "        return x,(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigdefenceModel(nn.Module):\n",
    "    def __init__(self,vocab_size=VOCAB_SIZE,emb_dim=1536,num_layers=12,n_query_heads=12,n_kv_heads=4,\n",
    "                 max_seq_len=2048):\n",
    "        super().__init__()\n",
    "        self.vocab_size=vocab_size\n",
    "        self.emb_dim=emb_dim\n",
    "        self.max_seq_len=max_seq_len\n",
    "        self.num_layers=num_layers\n",
    "\n",
    "        self.tok_emb=nn.Embedding(vocab_size,emb_dim)\n",
    "        self.drop_emb=nn.Dropout(0.1)\n",
    "\n",
    "        self.blocks=nn.ModuleList([\n",
    "            TransformerBlock(emb_dim,n_query_heads,n_kv_heads,max_seq_len) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.final_norm=RMSNorm(emb_dim)\n",
    "        self.lm_head=nn.Linear(emb_dim,vocab_size,bias=False)\n",
    "    def forward(self,x,past_kv=None,use_cache=False):\n",
    "        b,seq_len=x.shape\n",
    "        x=self.tok_emb(x)\n",
    "        x=self.drop_emb(x)\n",
    "        new_past_kv=[]\n",
    "        for i,block in enumerate(self.blocks):\n",
    "            past=None\n",
    "            if past_kv is not None:\n",
    "                past=past_kv[i]\n",
    "            x, (k,v) = block(x,past_kv=past,use_cache=use_cache)\n",
    "            new_past_kv.append((k,v))\n",
    "        x=self.final_norm(x)\n",
    "        logits=self.lm_head(x)\n",
    "        if use_cache:\n",
    "            return logits,new_past_kv\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    txt_file='korean_webtext_cleaned.txt'\n",
    "    with open(txt_file, 'r', encoding='utf-8') as f:\n",
    "        txt=f.read()\n",
    "    dataset=MyDataset(txt,max_len=2048,stride=128)\n",
    "    train_loader=DataLoader(dataset,batch_size=8,shuffle=True)\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model=BigdefenceModel(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        emb_dim=1536,\n",
    "        num_layers=12,\n",
    "        n_query_heads=12,\n",
    "        n_kv_heads=4,\n",
    "        max_seq_len=2048\n",
    "    ).to(device)\n",
    "    optimizer=torch.optim.AdamW(model.parameters(),lr=1e-4)\n",
    "    scheduler=torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=len(train_loader)*2)\n",
    "    criterion=nn.CrossEntropyLoss()\n",
    "    EPOCHS=1\n",
    "    global_step=0\n",
    "    scaler=GradScaler()\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        running_loss=0.0\n",
    "        for batch_idx, (input_batch,target_batch) in enumerate(train_loader):\n",
    "            input_batch=input_batch.to(device)\n",
    "            target_batch=target_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                logits=model(input_batch)\n",
    "                loss=criterion(logits.view(-1,logits.size(-1)),target_batch.view(-1))\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            running_loss+=loss.item()\n",
    "            global_step+=1\n",
    "            if global_step%100==0:\n",
    "                print(f\"Epoch {epoch+1}/{EPOCHS}, Step {global_step}, Loss: {running_loss/100}\")\n",
    "                running_loss=0.0\n",
    "        torch.save(model.state_dict(),f\"model_{epoch+1}.pt\")\n",
    "    return model\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_cache(\n",
    "        model,idx,max_new_tokens=256,temperature=0.8,top_k=40,eos_id=None\n",
    "):\n",
    "    model.eval()\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    idx=idx.to(device)\n",
    "    past_kv=None\n",
    "    with torch.no_grad():\n",
    "        for step in range(max_new_tokens):\n",
    "            logits,new_past_kv=model(idx[:,-1:],past_kv=past_kv,use_cache=True)\n",
    "            logits_last=logits[:,-1,:]\n",
    "            if top_k is not None:\n",
    "                v,_=torch.topk(logits_last,top_k)\n",
    "                threshold=v[:,-1]\n",
    "                logits_last[logits_last<threshold]=float('-inf')\n",
    "            probs=F.softmax(logits_last/temperature,dim=-1)\n",
    "            next_token=torch.multinomial(probs,num_samples=1)\n",
    "            if eos_id is not None and next_token.item()==eos_id:\n",
    "                break\n",
    "            idx=torch.cat((idx,next_token),dim=1)\n",
    "            past_kv=new_past_kv\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_chatbot(model):\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(\"간단한 챗봇을 시작합니다. 종료하려면 'exit'를 입력하세요.\")\n",
    "    while True:\n",
    "        user_input=input(\"User > \")\n",
    "        if user_input=='exit':\n",
    "            break\n",
    "        tokens=tokenizer(user_input)\n",
    "        input_ids=torch.tensor(tokens,dtype=torch.long).unsqueeze(0).to(device)\n",
    "        output_ids=generate_with_cache(\n",
    "            model,\n",
    "            input_ids,\n",
    "            max_new_tokens=256,\n",
    "            temperature=0.8,\n",
    "            top_k=40,\n",
    "            eos_id=None\n",
    "        )\n",
    "        response=tokenizer.decode(output_ids[0].tolist())\n",
    "        print(f\"Bot > {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_chatbot(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
